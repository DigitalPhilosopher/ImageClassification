{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "os.chdir(\"..\") # Changing to parent directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data/\"\n",
    "MODEL = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory: str) -> Tuple:\n",
    "    \"\"\"Load images from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory path.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: Images, image paths, labels and class to label mapping.\n",
    "    \"\"\"\n",
    "    # class_folders = [dir_ for dir_ in os.listdir(directory) if dir_ != \".DS_Store\"]\n",
    "    #TODO: class_folders = [\"email\", \"invoice\", \"letter\", \"resume\"]\n",
    "    class_to_label = {class_folder: index for index, class_folder in enumerate(class_folders)}\n",
    "\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for class_folder in class_folders:\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        for image_filename in os.listdir(class_path):\n",
    "            if image_filename == \".DS_Store\":# TODO\n",
    "                continue\n",
    "            image_path = os.path.join(class_path, image_filename)\n",
    "            # Open the image and convert it to grayscale if needed\n",
    "            image = Image.open(image_path).convert(\"L\")  # \"L\" mode converts to grayscale\n",
    "            images.append(image)\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(class_to_label[class_folder])\n",
    "    return images, image_paths, labels, class_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(\n",
    "        image_paths: List,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_masks: torch.Tensor, \n",
    "        val_size: float = 0.2, \n",
    "        test_size: float = 0.2\n",
    "    ) -> Tuple: \n",
    "    \"\"\"Split the image paths, input_ids, labels, and attention_masks into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: Image paths.\n",
    "        input_ids: The input ids.\n",
    "        labels: The labels.\n",
    "        attention_masks: The attention masks.\n",
    "        val_size: The proportion of the dataset to include in the validation split.\n",
    "        test_size: The proportion of the dataset to include in the test split.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: The train, val, and test data in the form of (image_paths, input_ids, labels, attention_masks).\n",
    "    \"\"\"\n",
    "    # adjust the validation size (because we split the data into train and test sets first)\n",
    "    val_size = val_size / (1 - test_size)\n",
    "\n",
    "    train_val_paths, test_paths, train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "        image_paths, input_ids, attention_masks, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "    )\n",
    "    train_paths, val_paths, train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        train_val_paths, train_val_inputs, train_val_masks, train_val_labels, test_size=val_size, stratify=train_val_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    train_data = (train_paths, train_inputs, train_labels, train_masks)\n",
    "    val_data = (val_paths, val_inputs, val_labels, val_masks)\n",
    "    test_data = (test_paths, test_inputs, test_labels, test_masks)\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_distribution(\n",
    "        all_labels: dict, \n",
    "        train_labels: dict, \n",
    "        val_labels: dict, \n",
    "        test_labels: dict\n",
    "    ) -> None:\n",
    "    \"\"\"Plot the label distribution of the dataset.\n",
    "    \n",
    "    Args:\n",
    "        all_labels: Labels of train, validation, and test sets.\n",
    "        train_labels: Train labels.\n",
    "        val_labels: Validation labels.\n",
    "        test_labels: Test labels.\n",
    "    \"\"\"\n",
    "    df_count = pd.DataFrame([all_labels, \n",
    "                            {label: list(train_labels.numpy()).count(label) for label in list(train_labels.numpy())}, \n",
    "                            {label: list(val_labels.numpy()).count(label) for label in list(val_labels.numpy())}, \n",
    "                            {label: list(test_labels.numpy()).count(label) for label in list(test_labels.numpy())}], \n",
    "                            index=[\"Total\", \"Train\", \"Validation\", \"Test\"])\n",
    "    fig = px.bar(df_count, barmode=\"stack\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_images(images: List[np.ndarray]) -> List[str]:\n",
    "    \"\"\"Extracts text from images using Tesseract OCR.\n",
    "    \n",
    "    Args:\n",
    "        images: List of images.\n",
    "    \n",
    "    Returns:\n",
    "        List: Extracted texts.\n",
    "    \"\"\"\n",
    "    extracted_texts = []\n",
    "    for image in images:\n",
    "        extracted_text = pytesseract.image_to_string(image)\n",
    "        extracted_text = extracted_text.replace(\"\\n\", \" \")\n",
    "        extracted_texts.append(extracted_text)\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset for BERT.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Tokenized input ids.\n",
    "        attention_masks: Attention masks.\n",
    "        labels: Labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "class BERTSequenceClassifier:\n",
    "    \"\"\"Wrapper for BERT sequence classification model.\n",
    "    \n",
    "    Args:\n",
    "        pretrained_model_name: Name of the pretrained model.\n",
    "        num_labels: Number of labels.\n",
    "        learning_rate: Learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_model_name: str = \"bert-base-uncased\",\n",
    "            num_labels: int = 4,\n",
    "            learning_rate: float = 1e-5\n",
    "        ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=num_labels)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            train_loader: DataLoader,\n",
    "            val_loader: DataLoader,\n",
    "            epochs: int = 10\n",
    "        ):\n",
    "        \"\"\"Train model.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: Train data loader.\n",
    "            val_loader: Validation data loader.\n",
    "            epochs: Number of epochs.\n",
    "        \"\"\"\n",
    "        writer = SummaryWriter(log_dir=f\"./logs/{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for batch in train_loader:\n",
    "                self._train_batch(batch)\n",
    "\n",
    "            val_loss = self.evaluate(val_loader)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "            writer.add_scalar(\"Loss/batch\", val_loss, epoch)\n",
    "\n",
    "    def _train_batch(self, batch: dict):\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, val_loader: DataLoader):\n",
    "        \"\"\"Evaluate model on validation set.\n",
    "        \n",
    "        Args:\n",
    "            val_loader: Validation data loader.\n",
    "        \n",
    "        Returns:\n",
    "            float: Validation loss.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        return val_loss\n",
    "\n",
    "    def save_model(self, path: str = \"bert_document_classification_model\"):\n",
    "        \"\"\"Save model to disk.\n",
    "        \n",
    "        Args:\n",
    "            path: Path to save model.\n",
    "        \"\"\"\n",
    "        self.model.save_pretrained(path)\n",
    "        print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_document_class(\n",
    "        model: BertForSequenceClassification,\n",
    "        tokenizer: BertTokenizer,\n",
    "        text: str\n",
    "    ) -> int:\n",
    "    \"\"\"Predict document class.\n",
    "\n",
    "    Args:\n",
    "        model: BERT model.\n",
    "        tokenizer: BERT tokenizer.\n",
    "        text: Text to classify.\n",
    "    \n",
    "    Returns:\n",
    "        int: Predicted class.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        model: BertForSequenceClassification, \n",
    "        test_loader: DataLoader,\n",
    "    ) -> Tuple:\n",
    "    \"\"\"Evaluate model.\n",
    "    \n",
    "    Args:\n",
    "        model: BERT model.\n",
    "        tokenizer: BERT tokenizer.\n",
    "        test_loader: Test dataloader.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Labels and predictions for the test set.\n",
    "    \"\"\"\n",
    "    # specify device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to calculate accuracy\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            # Update accuracy metrics\n",
    "            total_correct += torch.sum(predictions == labels).item()\n",
    "            total_samples += len(labels)\n",
    "\n",
    "            # Save labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # calculate f1 score (not needed here as we have a balanced dataset)\n",
    "    # f1 = f1_score(all_labels, all_predictions, average=\"weighted\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "\n",
    "    return all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrongly_classified_docs(paths: List[str], labels: List[int], pred: List[int], class_to_label: dict) -> List[str]:\n",
    "    \"\"\"Get wrongly classified images.\n",
    "    \n",
    "    Args:\n",
    "        paths: Image paths.\n",
    "        labels: True labels.\n",
    "        pred: Predicted labels.\n",
    "        class_to_label: Class to label mapping.\n",
    "    \n",
    "    Returns:\n",
    "        List: Paths of wrongly classified images.    \n",
    "    \"\"\"\n",
    "    wrongly_classified_docs = []\n",
    "    for i, (label, prediction) in enumerate(zip(labels, pred)):\n",
    "        if label != prediction:\n",
    "            label_name = [k for k, v in class_to_label.items() if v == label][0]\n",
    "            pred_name = [k for k, v in class_to_label.items() if v == prediction][0]\n",
    "            print(f\"{paths[i]} is <{label_name}>, but was predicted <{pred_name}>\")\n",
    "            wrongly_classified_docs.append(paths[i])\n",
    "\n",
    "    return wrongly_classified_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, image_paths, labels, class_to_label = load_images_from_directory(data_directory)\n",
    "label_count = {label: labels.count(label) for label in labels}\n",
    "extracted_texts = extract_text_from_images(images)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "tokenized_texts = tokenizer(extracted_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_data, val_data, test_data = create_train_val_test_split(\n",
    "    image_paths,\n",
    "    tokenized_texts.input_ids, \n",
    "    torch.tensor(labels), \n",
    "    tokenized_texts.attention_mask, \n",
    "    val_size=0.2, \n",
    "    test_size=0.2\n",
    ")\n",
    "train_paths, train_inputs, train_labels, train_masks = train_data\n",
    "val_paths, val_inputs, val_labels, val_masks = val_data\n",
    "test_paths, test_inputs, test_labels, test_masks = test_data\n",
    "\n",
    "plot_label_distribution(label_count, train_labels, val_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_inputs, val_masks, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_inputs, test_masks, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = BERTSequenceClassifier(pretrained_model_name=MODEL, num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img = Image.open(\"\")\n",
    "img_text = pytesseract.image_to_string(img)\n",
    "predicted_class = predict_document_class(model, tokenizer, img_text)\n",
    "predicted_label = [k for k, v in class_to_label.items() if v == predicted_class]\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, test_pred = evaluate_model(model=model, test_loader=test_loader)\n",
    "wrongly_classified_docs = get_wrongly_classified_docs(test_paths, test_labels, test_pred, class_to_label)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
